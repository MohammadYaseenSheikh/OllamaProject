# Ollama AI Configuration
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama3.2
spring.ai.ollama.chat.options.temperature=0.7

# Server Configuration (optional)
server.port=8080
spring.application.name=spring-ai-ollama-app

# Logging (optional, for debugging)
logging.level.org.springframework.ai=DEBUG
